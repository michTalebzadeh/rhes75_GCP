
# https://cloud.google.com/architecture/spark-on-kubernetes-engine

# Create a Kubernetes Engine cluster

gcloud config set compute/zone europe-west2-c
gcloud container clusters create spark-on-gke --machine-type n1-standard-2
# Configure identity and access management
gcloud iam service-accounts create spark-bq --display-name spark-bq

#Store the service account email address and your current project ID in environment variables to be used in later commands:

export SA_EMAIL=$(gcloud iam service-accounts list --filter="displayName:spark-bq" --format='value(email)')
export PROJECT=$(gcloud info --format='value(config.project)')

# The sample application must create and manipulate BigQuery datasets and tables and remove artifacts from Cloud Storage. Bind the bigquery.dataOwner, bigQuery.jobUser, and storage.admin roles to the service account:

gcloud projects add-iam-policy-binding $PROJECT --member serviceAccount:$SA_EMAIL --role roles/storage.admin
gcloud projects add-iam-policy-binding $PROJECT --member serviceAccount:$SA_EMAIL --role roles/bigquery.dataOwner
gcloud projects add-iam-policy-binding $PROJECT --member serviceAccount:$SA_EMAIL --role roles/bigquery.jobUser

# Download the service account JSON key and store it in a Kubernetes secret. Your Spark drivers and executors use this secret to authenticate with BigQuery:

gcloud iam service-accounts keys create spark-sa.json --iam-account $SA_EMAIL
kubectl create secret generic spark-sa --from-file=spark-sa.json

# Add permissions for Spark to be able to launch jobs in the Kubernetes cluster.

kubectl create clusterrolebinding user-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account)
kubectl create clusterrolebinding --clusterrole=cluster-admin --serviceaccount=default:default spark-admin


Create a Cloud Storage bucket to store the application jar and the results of your Spark pipeline:


export PROJECT=$(gcloud info --format='value(config.project)')
gsutil mb gs://$PROJECT-spark-on-k8s
Upload the application jar to the Cloud Storage bucket:


gsutil cp target/github-insights-1.0-SNAPSHOT-jar-with-dependencies.jar \
               gs://$PROJECT-spark-on-k8s/jars/
Create a new BigQuery dataset:


bq mk --project_id $PROJECT spark_on_k8s
Download the official Spark 2.3 distribution and unarchive it:


wget https://archive.apache.org/dist/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz
tar xvf spark-2.3.0-bin-hadoop2.7.tgz
cd spark-2.3.0-bin-hadoop2.7
Configure your Spark application by creating a properties file that contains your project-specific information:


cat > properties << EOF
spark.app.name  github-insights
spark.kubernetes.namespace default
spark.kubernetes.driverEnv.GCS_PROJECT_ID $PROJECT
spark.kubernetes.driverEnv.GOOGLE_APPLICATION_CREDENTIALS /mnt/secrets/spark-sa.json
spark.kubernetes.container.image gcr.io/cloud-solutions-images/spark:v2.3.0-gcs
spark.kubernetes.driver.secrets.spark-sa  /mnt/secrets
spark.kubernetes.executor.secrets.spark-sa /mnt/secrets
spark.driver.cores 0.1
spark.executor.instances 3
spark.executor.cores 1
spark.executor.memory 512m
spark.executorEnv.GCS_PROJECT_ID    $PROJECT
spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS /mnt/secrets/spark-sa.json
spark.hadoop.google.cloud.auth.service.account.enable true
spark.hadoop.google.cloud.auth.service.account.json.keyfile /mnt/secrets/spark-sa.json
spark.hadoop.fs.gs.project.id $PROJECT
EOF
Run the Spark application on the sample GitHub dataset by using the following commands:


export KUBERNETES_MASTER_IP=$(gcloud container clusters list --filter name=spark-on-gke --format='value(MASTER_IP)')
bin/spark-submit \
--properties-file properties \
--deploy-mode cluster \
--class spark.bigquery.example.github.NeedingHelpGoPackageFinder \
--master k8s://https://$KUBERNETES_MASTER_IP:443 \
--jars http://central.maven.org/maven2/com/databricks/spark-avro_2.11/4.0.0/spark-avro_2.11-4.0.0.jar \
gs://$PROJECT-spark-on-k8s/jars/github-insights-1.0-SNAPSHOT-jar-with-dependencies.jar \
$PROJECT spark_on_k8s $PROJECT-spark-on-k8s --usesample

gcloud container clusters resize spark-on-gke --num-nodes=0